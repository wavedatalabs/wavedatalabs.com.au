[
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Dean Marchiori",
    "section": "",
    "text": "The Team\nSmall but effective.\n\n  \n    \n\n    \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     CV\n  \n\n  \n  \nDean Marchiori is Director and Principal Data Scientist at Wave Data Labs where he consults on statistical modelling, applied mathematics and advanced analytics. Dean holds a BSc. in Mathematics with University Medal from Charles Sturt University, a Master of Applied Finance degree, and a Master of Applied Statistics from Macquarie University where he was awarded the Julian Leslie Prize in Statistics. He has been named one of the top 10 analytics leaders in Australia by the Institute of Analytics Professionals of Australia (IAPA). He is also recognised as an Accredited Statistician with the Statistical Society of Australia where he is co-chair of the committee for Statistical Computing and Visualisation.\nFor more info you can go to deanmarchiori.com"
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Wave Data Labs is committed to providing quality services to you and this policy outlines our ongoing obligations to you in respect of how we manage your Personal Information.\nWe have adopted the Australian Privacy Principles (APPs) contained in the Privacy Act 1988 (Cth) (the Privacy Act). The NPPs govern the way in which we collect, use, disclose, store, secure and dispose of your Personal Information. A copy of the Australian Privacy Principles may be obtained from the website of The Office of the Australian Information Commissioner at https://www.oaic.gov.au/.\n\n\nPersonal Information is information or an opinion that identifies an individual. Examples of Personal Information we collect includes names, addresses, email addresses, and phone numbers. This Personal Information is obtained in many ways including email, via our website www.wavedatalabs.com.au, from your website, services we provide and from third parties. We don’t guarantee website links or policy of authorised third parties.\nWe collect your Personal Information for the primary purpose of providing our services to you, providing information to our clients and marketing. We may also use your Personal Information for secondary purposes closely related to the primary purpose, in circumstances where you would reasonably expect such use or disclosure. You may unsubscribe from our mailing/marketing lists at any time by contacting us in writing.\nWhen we collect Personal Information we will, where appropriate and where possible, explain to you why we are collecting the information and how we plan to use it.\n\n\n\nSensitive information is defined in the Privacy Act to include information or opinion about such things as an individual’s racial or ethnic origin, political opinions, membership of a political association, religious or philosophical beliefs, membership of a trade union or other professional body, criminal record or health information.\nSensitive information will be used by us only:\n\nFor the primary purpose for which it was obtained\nFor a secondary purpose that is directly related to the primary purpose\n\nWith your consent; or where required or authorised by law.\n\n\n\n\nWhere reasonable and practicable to do so, we will collect your Personal Information only from you. However, in some circumstances we may be provided with information by third parties. In such a case we will take reasonable steps to ensure that you are made aware of the information provided to us by the third party.\n\n\n\nYour Personal Information may be disclosed in a number of circumstances including the following:\n\nThird parties where you consent to the use or disclosure; and\n\nWhere required or authorised by law.\n\n\n\n\nYour Personal Information is stored in a manner that reasonably protects it from misuse and loss and from unauthorized access, modification or disclosure. When your Personal Information is no longer needed for the purpose for which it was obtained, we will take reasonable steps to destroy or permanently de-identify your Personal Information. However, most of the Personal Information is or will be stored in client files which will be kept by us for a minimum of 7 years.\n\n\n\nYou may access the Personal Information we hold about you and to update and/or correct it, subject to certain exceptions. If you wish to access your Personal Information, please contact us in writing. Wave Data Labs will not charge any fee for your access request, but may charge an administrative fee for providing a copy of your Personal Information. In order to protect your Personal Information we may require identification from you before releasing the requested information.\n\n\n\nIt is an important to us that your Personal Information is up to date. We will take reasonable steps to make sure that your Personal Information is accurate, complete and up-to-date. If you find that the information we have is not up to date or is inaccurate, please advise us as soon as practicable so we can update our records and ensure we can continue to provide quality services to you.\n\n\n\nThis Policy may change from time to time and is available on our website.\n\n\n\nIf you have any queries or complaints about our Privacy Policy please contact us at:\ninfo@wavedatalabs.com.au"
  },
  {
    "objectID": "privacy.html#what-is-personal-information-and-why-do-we-collect-it",
    "href": "privacy.html#what-is-personal-information-and-why-do-we-collect-it",
    "title": "Privacy Policy",
    "section": "",
    "text": "Personal Information is information or an opinion that identifies an individual. Examples of Personal Information we collect includes names, addresses, email addresses, and phone numbers. This Personal Information is obtained in many ways including email, via our website www.wavedatalabs.com.au, from your website, services we provide and from third parties. We don’t guarantee website links or policy of authorised third parties.\nWe collect your Personal Information for the primary purpose of providing our services to you, providing information to our clients and marketing. We may also use your Personal Information for secondary purposes closely related to the primary purpose, in circumstances where you would reasonably expect such use or disclosure. You may unsubscribe from our mailing/marketing lists at any time by contacting us in writing.\nWhen we collect Personal Information we will, where appropriate and where possible, explain to you why we are collecting the information and how we plan to use it."
  },
  {
    "objectID": "privacy.html#sensitive-information",
    "href": "privacy.html#sensitive-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "Sensitive information is defined in the Privacy Act to include information or opinion about such things as an individual’s racial or ethnic origin, political opinions, membership of a political association, religious or philosophical beliefs, membership of a trade union or other professional body, criminal record or health information.\nSensitive information will be used by us only:\n\nFor the primary purpose for which it was obtained\nFor a secondary purpose that is directly related to the primary purpose\n\nWith your consent; or where required or authorised by law."
  },
  {
    "objectID": "privacy.html#third-parties",
    "href": "privacy.html#third-parties",
    "title": "Privacy Policy",
    "section": "",
    "text": "Where reasonable and practicable to do so, we will collect your Personal Information only from you. However, in some circumstances we may be provided with information by third parties. In such a case we will take reasonable steps to ensure that you are made aware of the information provided to us by the third party."
  },
  {
    "objectID": "privacy.html#disclosure-of-personal-information",
    "href": "privacy.html#disclosure-of-personal-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "Your Personal Information may be disclosed in a number of circumstances including the following:\n\nThird parties where you consent to the use or disclosure; and\n\nWhere required or authorised by law."
  },
  {
    "objectID": "privacy.html#security-of-personal-information",
    "href": "privacy.html#security-of-personal-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "Your Personal Information is stored in a manner that reasonably protects it from misuse and loss and from unauthorized access, modification or disclosure. When your Personal Information is no longer needed for the purpose for which it was obtained, we will take reasonable steps to destroy or permanently de-identify your Personal Information. However, most of the Personal Information is or will be stored in client files which will be kept by us for a minimum of 7 years."
  },
  {
    "objectID": "privacy.html#access-to-your-personal-information",
    "href": "privacy.html#access-to-your-personal-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "You may access the Personal Information we hold about you and to update and/or correct it, subject to certain exceptions. If you wish to access your Personal Information, please contact us in writing. Wave Data Labs will not charge any fee for your access request, but may charge an administrative fee for providing a copy of your Personal Information. In order to protect your Personal Information we may require identification from you before releasing the requested information."
  },
  {
    "objectID": "privacy.html#maintaining-the-quality-of-your-personal-information",
    "href": "privacy.html#maintaining-the-quality-of-your-personal-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "It is an important to us that your Personal Information is up to date. We will take reasonable steps to make sure that your Personal Information is accurate, complete and up-to-date. If you find that the information we have is not up to date or is inaccurate, please advise us as soon as practicable so we can update our records and ensure we can continue to provide quality services to you."
  },
  {
    "objectID": "privacy.html#policy-updates",
    "href": "privacy.html#policy-updates",
    "title": "Privacy Policy",
    "section": "",
    "text": "This Policy may change from time to time and is available on our website."
  },
  {
    "objectID": "privacy.html#privacy-policy-complaints-and-enquiries",
    "href": "privacy.html#privacy-policy-complaints-and-enquiries",
    "title": "Privacy Policy",
    "section": "",
    "text": "If you have any queries or complaints about our Privacy Policy please contact us at:\ninfo@wavedatalabs.com.au"
  },
  {
    "objectID": "services/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "href": "services/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "title": "Building your own R Data Science Lab in the browser",
    "section": "",
    "text": "Most R users probably just run RStudio Desktop from Posit on their local computers. This involves manually installing R, RStudio and all other packages.\nHowever it is often the case that users are operating in a restricted computing environment, such as in a corporate or government setting. Alternatively you may wish to create a custom development environment to test or replicate some other specific setup. This is a good case to move away from locally managed software to containerization, such as Docker.\nI have set up a Github repository that sets up a local data science development environment in the browser.\nIt builds a docker container including:\n\nUbuntu 20.04 LTS\nR version 4.2\nRStudio Server 2022.02.3+492\nAll tidyverse packages and devtools\ntex & publishing-related package\n\nThe image builds on the rocker/verse image from Rocker Project.\nSome other enhanced configuration options are included in the Dockerfile, such as preloading you RStudio preferences to get the same look and feel you have locally, the option to install other CRAN packages & mounting local volumes to persist your work locally.\nGo here for Step by step instructions:\nhttps://github.com/deanmarchiori/ds-env-setup"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "title": "Running Shiny in a Docker container",
    "section": "",
    "text": "Basic minimal example for running shiny in Docker. It is assumed you have Docker installed."
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "title": "Running Shiny in a Docker container",
    "section": "Dockerfile",
    "text": "Dockerfile\nThis should be adapted as required.\n# Using rocker/rver::version, update version as appropriate\nFROM rocker/r-ver:3.5.0\n\n# install dependencies\nRUN apt-get update && apt-get install -y \\\n    sudo \\\n    gdebi-core \\\n    pandoc \\\n    pandoc-citeproc \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\  \n    libxml2-dev \\  \n    libssl-dev \\  \n    wget\n\n\n# Download and install shiny server\nRUN wget --no-verbose https://download3.rstudio.org/ubuntu-14.04/x86_64/VERSION -O \"version.txt\" && \\\n    VERSION=$(cat version.txt)  && \\\n    wget --no-verbose \"https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-$VERSION-amd64.deb\" -O ss-latest.deb && \\\n    gdebi -n ss-latest.deb && \\\n    rm -f version.txt ss-latest.deb && \\\n    . /etc/environment && \\\n    R -e \"install.packages(c('shiny', 'rmarkdown'), repos='$MRAN')\" && \\\n    cp -R /usr/local/lib/R/site-library/shiny/examples/* /srv/shiny-server/\n\n# Copy configuration files into the Docker image\nCOPY shiny-server.conf  /etc/shiny-server/shiny-server.conf\nCOPY shiny-server.sh /usr/bin/shiny-server.sh\n\n# Copy shiny app to Docker image\nCOPY /myapp /srv/shiny-server/myapp\n\n# Expose desired port\nEXPOSE 80\n\nCMD [\"/usr/bin/shiny-server.sh\"]"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "title": "Running Shiny in a Docker container",
    "section": "List Images",
    "text": "List Images\ndocker images"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "title": "Running Shiny in a Docker container",
    "section": "List All Containers",
    "text": "List All Containers\ndocker ps -a"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "title": "Running Shiny in a Docker container",
    "section": "Remove Containers",
    "text": "Remove Containers\nFor individual containers add the container ID\n$ docker rm\nTo remove all exited containers :\n$ docker rm $(docker ps -a -q -f status=exited)"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "title": "Running Shiny in a Docker container",
    "section": "System Prune",
    "text": "System Prune\nRemove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.\ndocker system prune -a"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Save as tar-archive",
    "text": "Save as tar-archive\ndocker save -o ~/myapp.tar myapp"
  },
  {
    "objectID": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "href": "services/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Load and Run Archive",
    "text": "Load and Run Archive\ndocker load -i myapp.tar\ndocker run myapp"
  },
  {
    "objectID": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "href": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "",
    "text": "In a previous post I linked to a project that makes it easy to deploy and extend an existing Rocker Project Docker image to quickly set up a fully featured RStudio Server environment locally on your machine.\nHere I’ll cover some options to deploy this environment to the cloud so you can access it anywhere."
  },
  {
    "objectID": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "href": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 1: Deploy to a Virtual Machine",
    "text": "Option 1: Deploy to a Virtual Machine\nA common pattern is to create a Virtual Machine (VM) with a cloud service provider (such as AWS, Azure, GCP) and run your code there. I’ll cover an example using Microsoft Azure.\n\nDeploy a VM with an Ubuntu operating system. Go ahead and choose the compute power you need.\n\n\n\nConfigure a custom network rule to allow traffic on port 8787 for RStudio\n\n\n3. Log into your new VM terminal using SSH\n\n\nInstall Docker Engine by following these steps\nClone and Deploy the docker container from Step 2 in my guide."
  },
  {
    "objectID": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "href": "services/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 2: Deploy using Azure App Service",
    "text": "Option 2: Deploy using Azure App Service\nThe above is fine, but arguably if you are setting up a VM from scratch for development purposes I’m not sure what benefit there is from using a docker container. You may as well just directly install what you want and consider the VM a ‘container’.\nHowever, if you plan to make this available to other users in your organisation, or to adapt this guide for Shiny App development you may be interested in other features such as TLS/SSL security, scale up, advanced networking, continuous integration, continuous deployment, staging/production deployment slots etc. This represents a shift from development sandpit to ‘web app’. For this case, Azure App Service may be a lower hassle option. This is Microsoft’s enterprise grade, web app deployment managed service.\nIn the Virtual Machine model you are setting up compute infrastructure, deploying and running containers directly - then fiddling with the infrastructure layer for everything else. In App Service you deploy your custom docker container (here containing RStudio Server) to Azure Container Registry (kind of like DockerHub). Azure App Services then builds and serves your app from there - without you having to stand up and manage an Infra layer directly.\n\n\nCreate Azure Container Registry (ACR) (or some other Docker repository) using this help guide\nRun and test your container locally\nDeploy your local container to ACR using this help guide\nCreate a new web app in Azure App Services using this help guide\nConfiguration:\n\n\nI didn’t have to fiddle with ports, presumably it reads the exposed ports in the docker file and does this magically.\n\nFor custom environment variables like the RStudio Server password, I had to manually add this in the config section.\n\n\nand it worked just fine:"
  },
  {
    "objectID": "services/index.html",
    "href": "services/index.html",
    "title": "Data Science & Statistical Consulting",
    "section": "",
    "text": "We are available for short or long term consulting and contract opportunities in a range of fields. If you are interesting in working together get in touch here."
  },
  {
    "objectID": "services/index.html#some-of-our-work",
    "href": "services/index.html#some-of-our-work",
    "title": "Data Science & Statistical Consulting",
    "section": "Some of our work",
    "text": "Some of our work"
  },
  {
    "objectID": "services/2023-06-20-dds/index.html#seeking-innovative-ideas",
    "href": "services/2023-06-20-dds/index.html#seeking-innovative-ideas",
    "title": "Data Science Discovery and Design Sprint",
    "section": "Seeking innovative ideas?",
    "text": "Seeking innovative ideas?\nOur Discovery phase allows you to explore, ideate, and prioritize data science use cases that align perfectly with your organizational goals and challenges."
  },
  {
    "objectID": "services/2023-06-20-dds/index.html#need-to-assess-your-capabilities",
    "href": "services/2023-06-20-dds/index.html#need-to-assess-your-capabilities",
    "title": "Data Science Discovery and Design Sprint",
    "section": "Need to assess your capabilities?",
    "text": "Need to assess your capabilities?\nOur Sprint includes a thorough assessment of your existing data science infrastructure, identifying strengths, weaknesses, and opportunities for growth."
  },
  {
    "objectID": "services/2023-06-20-dds/index.html#ready-to-take-action",
    "href": "services/2023-06-20-dds/index.html#ready-to-take-action",
    "title": "Data Science Discovery and Design Sprint",
    "section": "Ready to take action?",
    "text": "Ready to take action?\nOur Design phase takes your chosen use cases and translates them into robust, actionable plans, leveraging cutting-edge data science techniques and tools."
  },
  {
    "objectID": "services/2023-06-20-dds/index.html#why-choose-us",
    "href": "services/2023-06-20-dds/index.html#why-choose-us",
    "title": "Data Science Discovery and Design Sprint",
    "section": "Why choose us?",
    "text": "Why choose us?\nWe bring together a team of seasoned data scientists, industry experts, and consultants who will guide you every step of the way, ensuring your success."
  },
  {
    "objectID": "services/2023-06-20-dds/index.html#book-a-demo",
    "href": "services/2023-06-20-dds/index.html#book-a-demo",
    "title": "Data Science Discovery and Design Sprint",
    "section": "Book a demo",
    "text": "Book a demo"
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "Let’s build something cool together!\n info@wavedatalabs.com.au\n Book a meeting\n\n\n\n\n\n Ground Floor, Enterprise 1, Innovation Campus, Squires Way, North Wollongong NSW 2500"
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html",
    "href": "posts/2023-06-26-failure/index.html",
    "title": "Why your data science projects are failing",
    "section": "",
    "text": "The most undervalued skill in delivering value with data science teams is picking projects that are likely to succeed. There is no shortcut - it takes years of hard earned experience.\nA number that seems to be floating around is 80% of data science projects will FAIL. Ouch.\nMany of these types of numbers are ‘predictions’ from consultancies who stand to benefit from making big claims.\nhttps://blogs.gartner.com/andrew_white/2019/01/03/our-top-data-and-analytics-predicts-for-2019/\nCited reasons to fix this include:\nThese are all lovely ideas, but moving the lever on these are often impossible or impractical."
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html#so-what-are-some-easy-things-you-can-you-do",
    "href": "posts/2023-06-26-failure/index.html#so-what-are-some-easy-things-you-can-you-do",
    "title": "Why your data science projects are failing",
    "section": "So what are some easy things you can you do?",
    "text": "So what are some easy things you can you do?\n\nChange your mindset (and how you run projects)\n\nData analytics is an exploratory and scientific endeavour that isn’t supposed to succeed every time. Just like not all lab experiments yield positive results. Instead of lamenting failures, develop a mindset of innovation and agile working where new ideas are prototyped and investment in R&D promoted but capped and balanced.\n\nPick better projects\n\nA question I get all the time, is how to get started with data science projects in an established business. Often there is a disconnect between those doing the work and those deciding what to do. The most undervalued skill in delivering value with data science teams is picking projects that are likely to succeed. There is no shortcut - it takes years of hard earned experience and it requires a balance of hands-on technical skills, with commercial awareness."
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html#how-can-we-help",
    "href": "posts/2023-06-26-failure/index.html#how-can-we-help",
    "title": "Why your data science projects are failing",
    "section": "How can we help?",
    "text": "How can we help?\nWe have a dedicated program for businesses looking to get started or deepen their data analytics journey. We can help change your attitude and pick better projects.\nBook in a Demo"
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html",
    "href": "posts/2023-05-18-RNG-Testing/index.html",
    "title": "Random Number Generator Testing",
    "section": "",
    "text": "We had really interesting discussion with a company recently who needed a random number generator certified.\nRandomness, like time or space, is one of these deep concepts that are super hard to reason about. Despite this, it’s fairly common to see random number generators in practice. A casino will use one in their gaming software to randomise outcomes; A lottery or competition website will use one to pick winners; Scientists use them to run simulations and cryptographic applications are powered by some form of randomness.\nFlash back to the 1980’s where down-on-his-luck unemployed ice cream man Michael Larsen cracked the (non-random) pattern in TV game show Press Your Luck and took them for over $100k. Sadly it didnt end well for Larsen with Ponzi schemes, radio station challenges awry and a break and enter. But it goes to show what can happen if you don’t take randomness seriously."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#aside-ok-but-what-is-random-and-who-is-ramsey",
    "href": "posts/2023-05-18-RNG-Testing/index.html#aside-ok-but-what-is-random-and-who-is-ramsey",
    "title": "Random Number Generator Testing",
    "section": "(Aside) Ok but what is random? And who is Ramsey?",
    "text": "(Aside) Ok but what is random? And who is Ramsey?\nRandomness is an actual or apparent lack of pattern, but it’s kind of hard to test and even its very nature is somewhat unclear. In 1903, a British mathematician called Frank Ramsey was born. Ramsey was a militant atheist, but interestingly his brother went on to become Archbishop of Canterbury. He went on to study mathematics and economics, becoming a student of famous economist John Maynard Keynes. Somehow he ended up also translating a German book of logical philosophy into English and joined a secret intellectual society just after the war. A minor discovery of his ended up blossoming into what is known as Ramsey theory, which is a theory in mathematical combinatorics showing that in any sufficiently large system, however disordered, there is always some order. This has had interesting (and conspiratorial) implications for whether there is even such a thing as ‘random’. Oh and by the way, all this happened before he died at age 26 after complications from liver disease likely caused by swimming in a river."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#types-of-random-number-generators-rngs",
    "href": "posts/2023-05-18-RNG-Testing/index.html#types-of-random-number-generators-rngs",
    "title": "Random Number Generator Testing",
    "section": "Types of Random Number Generators (RNGs)",
    "text": "Types of Random Number Generators (RNGs)\nGenerally RNG’s can generate True random numbers or Pseudo random numbers. True RNGs generate random bits from natural stochastic sources like background radiation, quantum effects, atmospheric noise etc. Next time you are tempted to toss a coin, perhaps head over to random.org instead for some ‘true’ randomness.\nThere is a fun history lesson for how random.org got started with true RNG’s generated using random static from a cheap $10 radio, laden with a post-it note advising passers by not to fiddle with the knobs.\n\n\n\nEarly true RNG using atmospheric noise from a cheap radio\n\n\nPseudo-random numbers are generated using a ‘seed’ that deterministically produces numbers that look random, but can be entirely reproduced from the initial seed condition. This is often useful (and used by me all the time) when you need a random sample, but you need it to replicated exactly for scientific reproducibility purposes."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#statistical-tests",
    "href": "posts/2023-05-18-RNG-Testing/index.html#statistical-tests",
    "title": "Random Number Generator Testing",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nGiven randomness itself is hard to test, there are a number of statistical test suites that perform a battery of diagnostics on a large sample of random numbers in order to test various aspects of randomness. One prominent test suite for cryptographic random bits is developed by NIST which uses 15 different statistical tests.\n\nThe Frequency (Monobit) Test\nFrequency Test within a Block\nThe Runs Test\nTests for the Longest-Run-of-Ones in a Block\nThe Binary Matrix Rank Test\nThe Discrete Fourier Transform (Spectral) Test\nThe Non-overlapping Template Matching Test\nThe Overlapping Template Matching Test\nMaurer’s “Universal Statistical” Test\nThe Linear Complexity Test\nThe Serial Test\nThe Approximate Entropy Test\nThe Cumulative Sums (Cusums) Test\nThe Random Excursions Test\nThe Random Excursions Variant Test\n\nSo, like much of the mathematics behind every day scenarios there is a fascinating history and deep technical and philosophical implications. Given what is on the line for organisations relying on randomness, its useful to engage a specialist to help run and interpret these test suites.\nAnd remember, if you get it wrong, someone unemployed ice-cream man is just waiting to swoop in and take advantage."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html",
    "href": "posts/2023-05-29-forecasting/index.html",
    "title": "Level up your Forecasting",
    "section": "",
    "text": "This is a time series data set of half-hourly electricity demand for Victoria, Australia 1. You may not care about electricity forecasting, but there is probably some similar time series in your organisation you do care about.\nWhat is the forecast demand for the next week?\nWho knows? I certainly don’t know, despite doing forecasting for a job.\nIn fact, anyone who tells you they do know (with certainty) is wrong."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#but-i-need-to-make-a-forecast-so-what-do-i-do",
    "href": "posts/2023-05-29-forecasting/index.html#but-i-need-to-make-a-forecast-so-what-do-i-do",
    "title": "Level up your Forecasting",
    "section": "But I need to make a forecast, so what do I do?",
    "text": "But I need to make a forecast, so what do I do?\nOften you have to come up with something. Finance needs an estimate of sales or budgets, or your manager needs something to take to the board. So you have to churn out some numbers. But what numbers?\nHere are two extremes:\n\nYou could ignore all historical data and just use the most recent data point as the most reliable forecast.\n\nYou could ignore any specific value and just take the average of everything.\n\n\n\n\n\n\nBoth are in the ballpark but they aren’t very squiggly like the real data. They clearly aren’t capturing the seasonality.\nYou could be lazy (clever) and just use the same time period from last week as your current forecast. It actually looks really good. But it’s just one, fixed realization of what could happen. Will it play out exactly like your forecast? Exactly like last week? Almost certainly not.\n\n\n\n\n\nLet’s simulate another way history could play out. (By statistically resampling the residuals in the training data)\n\n\n\n\n\nAnd another\n\n\n\n\n\nAnd 20 more:\n\n\n\n\n\nSo instead of blindly relying on one (kind of okay looking, but totally unrealistic) forecast, we now have a ‘fuzzy’ region of plausible values."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#ok-but-why-bother",
    "href": "posts/2023-05-29-forecasting/index.html#ok-but-why-bother",
    "title": "Level up your Forecasting",
    "section": "Ok but why bother?",
    "text": "Ok but why bother?\nIf you don’t know with certainty what your forecast is going to be, then don’t just give one concrete number. It’s misleading.\n\nIts more important to know how uncertain your forecast is rather than what your forecast is.\n\nYou can still pull out a mean or point estimate but by delivering the whole story you are conveying not just what you think is likely to happen, but how certain you are about it.\nOften this can lead to more meaningful discussions. Perhaps it’s not the mean of your forecast distribution that you care about, its the extreme values. For example, If I was stress testing business cash flows and forecasting the cost of a maintenance activity, I’d be more interested in forecasting the 95% upper limit of forecast costs rather than the ‘expected’ cost.\nIn practice, you often don’t have to do any mathematical simulations. Proper time series forecasts have methods to calculate prediction intervals out of the box (see below)."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#spoiler",
    "href": "posts/2023-05-29-forecasting/index.html#spoiler",
    "title": "Level up your Forecasting",
    "section": "Spoiler",
    "text": "Spoiler\nWant to know what actually happened? Here it is in red.\n\n\n\n\n\nA huge criticism of providing probabilistic forecasts is that is seems like you are ‘hedging your bets’ and being evasive. The reality is, in our electricity example, it was just very hot and demand surged (much higher than even our 95% prediction interval). So if it’s realistic and plausible to see forecast values in these ranges (or even more extreme) - Why wouldn’t you want to include that information in your forecast??"
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#aside",
    "href": "posts/2023-05-29-forecasting/index.html#aside",
    "title": "Level up your Forecasting",
    "section": "Aside",
    "text": "Aside\nIn practice a statistician will likely use a more sophisticated model than presented here. These models may take into account temperature and other factors but there will still be unexplained variance that will need to be quantified if you want a quality forecast produced.\nIf you or your organisation want to get serious about making proper forecasts and being proactive when making critical decisions - drop me a line, I can help."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#references",
    "href": "posts/2023-05-29-forecasting/index.html#references",
    "title": "Level up your Forecasting",
    "section": "References",
    "text": "References\nO’Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‘tsibble’. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#footnotes",
    "href": "posts/2023-05-29-forecasting/index.html#footnotes",
    "title": "Level up your Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Australian Energy Market Operator. tsibbledata R package.↩︎"
  },
  {
    "objectID": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "href": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "title": "Building your own R Data Science Lab in the browser",
    "section": "",
    "text": "Most R users probably just run RStudio Desktop from Posit on their local computers. This involves manually installing R, RStudio and all other packages.\nHowever it is often the case that users are operating in a restricted computing environment, such as in a corporate or government setting. Alternatively you may wish to create a custom development environment to test or replicate some other specific setup. This is a good case to move away from locally managed software to containerization, such as Docker.\nI have set up a Github repository that sets up a local data science development environment in the browser.\nIt builds a docker container including:\n\nUbuntu 20.04 LTS\nR version 4.2\nRStudio Server 2022.02.3+492\nAll tidyverse packages and devtools\ntex & publishing-related package\n\nThe image builds on the rocker/verse image from Rocker Project.\nSome other enhanced configuration options are included in the Dockerfile, such as preloading you RStudio preferences to get the same look and feel you have locally, the option to install other CRAN packages & mounting local volumes to persist your work locally.\nGo here for Step by step instructions:\nhttps://github.com/deanmarchiori/ds-env-setup"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "",
    "text": "We have a manufacturing process in the day job that is subject to sample auditing.\nThere are \\(N\\) widgets produced and we need to audit \\(n\\) of them. Some sort of rejection threshold is needed on that sample to decide if the whole batch of widgets has met a specified quality level.\nTypically, a binomial distribution would be appropriate for measuring the probability of \\(k\\) successes (in this case defects found) in \\(n\\) independent trials with probability \\(p\\).\n\\[\nPr(X=k) = {n \\choose k} p^k(1-p)^{n-k}\n\\]\nThe word independent is doing a lot of work here as it implies that we are sampling with replacement in order to maintain a fixed probability parameter \\(p\\).\nIn cases where you are taking draws from a population without replacement (such as when you do destructive inspections on a widget) the underlying population changes with each draw and so does the probability \\(p\\).\nIn this case, modelling the process using a hypergeometric distribution may be a more appropriate choice.\n\\[\nPr(X=k) = \\frac{{K \\choose k}{N-K \\choose n-k}}{{N \\choose n}}\n\\]\nIt similarly describes the probability of \\(k\\) successes in \\(n\\) draws without replacement. However, instead of specifying a parameter \\(p\\), we provide the population size \\(N\\), which contains \\(K\\) success states in the population."
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Example",
    "text": "Example\nLet’s say we have 2000 widgets manufactured and we want to sample 50 (ignore why 50, that is a whole separate question). We have an assumed quality level of 10% defective units (which we define as ‘success’ for complicated reasons).\nQ: Based on a sample of 50 widgets how many defective units would be considered unlikely (95% CI) to occur randomly given our assumed quality level, and therefore result in us rejecting the entire batch?\nWe can compare the binomial probability mass function with the hypergeometric and observe they are essentially the same.\n\nlibrary(tidyverse)\n\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\nHowever, if we had a smaller population of say 100 or 70 widgets, how would this compare?\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       hypergeom_100 = dhyper(x, m = 10, n = 90, k = 50),\n       hypergeom_070 = dhyper(x, m = 7, n = 63, k = 50)\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\nWe can see these curves are markedly different. And indeed the 95% confidence intervals obtained are narrower for the hypergeometric case.\n\nqbinom(p = c(0.025, 0.975), size = 50, prob = 0.1)\n\n[1] 1 9\n\nqhyper(p = c(0.025, 0.975), m = 10, n = 90, k = 50)\n\n[1] 2 8\n\n\nWe can see from a random draw of 1 million samples from each PMF that they both have the same expected values, but the variance is smaller in the hypergeometric case.\n\nX &lt;- rbinom(n = 1e6, size = 50, prob = 0.1)\nY &lt;- rhyper(nn = 1e6, m = 10, n = 90, k = 50)\n\nmean(X)\n\n[1] 4.999079\n\nvar(X)\n\n[1] 4.498633\n\nmean(Y)\n\n[1] 5.000942\n\nvar(Y)\n\n[1] 2.273265"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Does it matter which one you use?",
    "text": "Does it matter which one you use?\nAs a consequence of removing samples in each draw we influence the probability of a subsequent success. If our \\(N\\) and \\(K\\) is very large relative to our sample \\(n\\) this wont make much of an impact, but it can be impactful for smaller populations, or relatively larger samples.\nFrom our example above, failing to use a hypergeometric distribution to model this process for smaller populations will result in wider, more conservative acceptance regions which can increase consumer risk in a manufacturing process.\nTypical guidance on when to use each distribution is given in manufacturing standards such as AS 1199.1-2003: Sampling Procedures for Inspection by Attributes and typically involves how you structure your sampling scheme."
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "title": "Running Shiny in a Docker container",
    "section": "",
    "text": "Basic minimal example for running shiny in Docker. It is assumed you have Docker installed."
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "title": "Running Shiny in a Docker container",
    "section": "Dockerfile",
    "text": "Dockerfile\nThis should be adapted as required.\n# Using rocker/rver::version, update version as appropriate\nFROM rocker/r-ver:3.5.0\n\n# install dependencies\nRUN apt-get update && apt-get install -y \\\n    sudo \\\n    gdebi-core \\\n    pandoc \\\n    pandoc-citeproc \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\  \n    libxml2-dev \\  \n    libssl-dev \\  \n    wget\n\n\n# Download and install shiny server\nRUN wget --no-verbose https://download3.rstudio.org/ubuntu-14.04/x86_64/VERSION -O \"version.txt\" && \\\n    VERSION=$(cat version.txt)  && \\\n    wget --no-verbose \"https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-$VERSION-amd64.deb\" -O ss-latest.deb && \\\n    gdebi -n ss-latest.deb && \\\n    rm -f version.txt ss-latest.deb && \\\n    . /etc/environment && \\\n    R -e \"install.packages(c('shiny', 'rmarkdown'), repos='$MRAN')\" && \\\n    cp -R /usr/local/lib/R/site-library/shiny/examples/* /srv/shiny-server/\n\n# Copy configuration files into the Docker image\nCOPY shiny-server.conf  /etc/shiny-server/shiny-server.conf\nCOPY shiny-server.sh /usr/bin/shiny-server.sh\n\n# Copy shiny app to Docker image\nCOPY /myapp /srv/shiny-server/myapp\n\n# Expose desired port\nEXPOSE 80\n\nCMD [\"/usr/bin/shiny-server.sh\"]"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "title": "Running Shiny in a Docker container",
    "section": "List Images",
    "text": "List Images\ndocker images"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "title": "Running Shiny in a Docker container",
    "section": "List All Containers",
    "text": "List All Containers\ndocker ps -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "title": "Running Shiny in a Docker container",
    "section": "Remove Containers",
    "text": "Remove Containers\nFor individual containers add the container ID\n$ docker rm\nTo remove all exited containers :\n$ docker rm $(docker ps -a -q -f status=exited)"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "title": "Running Shiny in a Docker container",
    "section": "System Prune",
    "text": "System Prune\nRemove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.\ndocker system prune -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Save as tar-archive",
    "text": "Save as tar-archive\ndocker save -o ~/myapp.tar myapp"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Load and Run Archive",
    "text": "Load and Run Archive\ndocker load -i myapp.tar\ndocker run myapp"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html",
    "href": "posts/2023-07-12-three-questions/index.html",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "",
    "text": "If you have hired a data scientist or run a team of data people, you may not be an expert yourself (that’s why you hired an expert, right?). So how do you know you are receiving a quality predictive model and not some BS that was thrown carelessly at a black-box machine learning model.\nThe consequences of poorly built statistical models are not trivial. In 2015, Amazon realized its ‘AI recruiting tool’ didn’t like women1. In the US, facial recognition models used by police were found to have biases that more commonly misidentified underrepresented communities.2\nSo what can you do about it?"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#three-questions-you-can-ask-your-data-scientist",
    "href": "posts/2023-07-12-three-questions/index.html#three-questions-you-can-ask-your-data-scientist",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "Three questions you can ask your data scientist",
    "text": "Three questions you can ask your data scientist\n\n1. How could the model be unfair?\n\nWhat training data was used, and how was this collected?\nCould certain sub-populations be over or under-represented in this data?\nHave customer details been anonymised?\n\nAre sensitive features used in the model training (race, religion, gender, political preferences).\n\nAlgorithmic unfairness and biases are already huge issues which are only getting worse. Further reading on this topic is covered by Cathy O’Neil’s excellent book Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy3.\n\n\n2. What assumptions does the model make, and how have these been tested?\nAll statistical models have assumptions that are made in order for the math to work out. Models are intended to be simplified representations of reality, deliberately so statisticians can exploit properties like the Central Limit Theorem to help make inferences. For example the assumptions behind a simple linear regression include:\n\nThe response variable can be expressed as a linear combination of the predictors.\n\nThe variance of the error terms is homoscedastic (has constant variance).\n\nThe errors in the response are independent.\n\nIn many cases these can be checked using standard diagnostics tests and plots. However in most cases it requires more in depth domain knowledge and context.\n\n\n3. How accurate is the model?\nEver been told a model is 99% accurate? I’d be very worried if you had. Be skeptical of very high performance.\nxkcd.com highlight this well with their ‘Is it Christmas?’ predictive model.\n\n\nOn what data has the model been tested? Is it a completely independent test set? Was there any leakage from the training data? Was the feature engineering done before or after the training/test split (hint: it usually needs to be after).\nIf your model is a binary classification model, you should know what the ‘null model’ is and whether it outperforms this.\nAccuracy is just one measure and is the proportion of correct classifications (both positive and negative class) but you may have different tolerance for misclassifications of each class. For example, you might be predicting the presence of a disease from a test. If you make a false positive, will the risk of side-effects or cost outweigh the risk the making a false negative and having the patient get sicker or die? It’s tricky.\nIn addition to accuracy ask for the following metrics, along with an explanation of what they mean.\n\nSensitivity / Recall\nSpecificity\nPrecision / Positive Predictive Value\nNegative Predictive Value\nROC AUC\n\nAsk for a confusion matrix. All of the above measures (except ROC) can be calculated from the confusion matrix. Despite its name, it will be instantly clear where the model is working well versus not.\nAsk what cut-off or threshold was used to make the predicted classifications. Many classification models return conditional class probabilities, which need to be converted into labels such as (Yes/No, Cancer/Not Cancer, Churn/Not Churn). A default value is to use a 0.5 probability for the crisp cutoff, but it’s subjective and depends on the desired trade off in sensitivity / specificity as well as other complicated factors like training class imbalance."
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#what-now",
    "href": "posts/2023-07-12-three-questions/index.html#what-now",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "What now?",
    "text": "What now?\nIf you don’t get good answers to these questions, you should probably give me a call.\nHave I missed something? Let me know!"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#footnotes",
    "href": "posts/2023-07-12-three-questions/index.html#footnotes",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G↩︎\nhttps://www.nytimes.com/2019/07/08/us/detroit-facial-recognition-cameras.html↩︎\nhttps://www.goodreads.com/book/show/28186015-weapons-of-math-destruction↩︎"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html",
    "href": "posts/2023-06-02-man-vs-ml/index.html",
    "title": "Man vs Machine Learning",
    "section": "",
    "text": "Over the past few years I have been doing more and more work in Microsoft Azure and the Azure Machine Learning Studio. One feature of the Azure ML studio is an automated machine learning feature. This is essentially a no-code UI that ‘empowers professional and nonprofessional data scientists to build machine learning models rapidly’ (emphasis mine).\nWhile many (including me) have leveled a fair amount of criticism towards such solutions, I thought it would be worth seeing what the fuss was about.\nCould I go head-to-head on the same predictive modelling challenge and compete with the might of Microsoft’s AutoML solution? Even worse, would I enjoy it? Even more worse, could I win??"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "href": "posts/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "title": "Man vs Machine Learning",
    "section": "Method 1: Azure AutoML",
    "text": "Method 1: Azure AutoML\nThe process to set up a new AutoML job was very easy and assumes you are working under somewhat sanitized conditions (which I was in this case).\n\nOnce you kick it off, it chugged away for an hour and 33 minutes. To my horror, I realized it takes the ‘kitchen sink’ approach and fits a suite of 41 (!) different machine learning models at the training data. Hyperparameter tuning is done by constructing a validation set using K-Fold cross validation.\nVideo\nThe best performing model is then selected and then predictions are run on the test set. It’s a little concerning that Test set evaluation is only in ‘Preview’ mode. It was also very confusing to dig out the results on the test set. Most of the metrics prominently displayed are overly confident in-sample accuracy results.\nThe winning model in my case was a ‘Voting Ensemble’ of three models\n\nMaxAbsScaler, ExtremeRandomTrees\nStandardScalerWrapper, XGBoostRegressor\n\nStandardScalerWrapper, LightGBM\n\nOverall the process was very easy and user friendly. It look a long time to train, but I didn’t have to think about anything - at all (which is usually time consuming) so overall it was a quick solution. I trained the model on a Standard_DS11_v2 (2 cores, 14 GB RAM, 28 GB disk) compute instance which costs $0.2 per hour. So it cost money, but not much.\nPerformance evaluation to follow below…"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "href": "posts/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "title": "Man vs Machine Learning",
    "section": "Method 2: Manual Time Series Model in R",
    "text": "Method 2: Manual Time Series Model in R\nThe process for doing this myself involved much more thought and brain-effort. Here are some notes.\nThe data set is quite complicated as its sub-daily and has (probably) three seasonal periods (daily, weekly, yearly). There was also maybe some trend and outliers to deal with. The data set also contained covariates such as Temperature and Holiday indicators.\nDue to the seasonal complexity many traditional statistical methods were not appropriate like straight ARIMA (autoregressive integrated moving average) and ETS (exponential smoothing). While STL (Seasonal and Trend decomposition using Loess) can handle multiple seasonal periods I wanted a method to handle the covariates (like Temperature and Holidays). My next step was to think of Time Series Linear Regression models. However, accounting for yearly seasonality with 30min data meant fitting 17,520 (2 * 24 * 365) parameters just for this seasonal period. Which seemed excessive.\nFor longer, multiple-seasonal periods, using Fourier terms can be a good idea. Here a smaller number of terms in a fourier series can be estimated to approximate a more complex function. This type of Dynamic Harmonic Regression2 can also handle exogenous covariates and we can even fit the model with ARIMA errors to account for the short term dynamics of time series data.\nIn fact, this very approach was outlined in the excellent Forecasting: Principles and Practice3 using this very same example data set. I decided to borrow (steal) the ideas of creating a piece-wise linear trend for temperature. I also went a bit crazy with encoding specific holiday dummy variables and some other tweaks.\nOverall I found this method slow to fit, and not overly performant. I decided next to try fitting a Prophet4 model. Prophet is an open-source automated algorithm for time series forecasting developed by Facebook. It uses a Bayesian framework to fit complex, non-linear, piece-wise regression models. For complex time series data, it provides a decent, fast framework including exogenous variables, holiday and seasonal effects. I didn’t do any principled hyperparameter tuning, but I did fiddle around with the model a bit."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#results",
    "href": "posts/2023-06-02-man-vs-ml/index.html#results",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nSo who won?\nThe AutoML platform did :( , but only just. Below is the comparison of RMSE and MAPE. The AutoML is red, my predictions are in blue. I stuffed up over Christmas a bit, which admittedly is a tricky hold-out month for testing.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n213\n\n\nAzure AutoML\nMAPE\n3.56\n\n\nMe\nRMSE\n274\n\n\nMe\nMAPE\n4.96"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#discussion",
    "href": "posts/2023-06-02-man-vs-ml/index.html#discussion",
    "title": "Man vs Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nSo overall it was pretty close, but in terms of pure predictive performance, the AutoML platform did pip me at the post. Admittedly, the solution I arrived at was probably more of an ML solution than a ‘classical’ time series method given it is still an automated algorithm. If I had more time and patience I probably could have pursued a more complex regression model. In fact in Forecasting: Principles and Practice, the authors also cite the performance of a straight Dynamic Harmonic Regression is limited, however they go on to propose other innovative approaches56, including splitting the problem into separate models for each 30min period and using regression splines to better capture exogenous effects. So it can be done, but not without a huge amount of effort."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#automl-solution",
    "href": "posts/2023-06-02-man-vs-ml/index.html#automl-solution",
    "title": "Man vs Machine Learning",
    "section": "AutoML solution",
    "text": "AutoML solution\nThe AutoML platform again used a Voting Ensemble, churned out in 43 minutes, but this time using:\n\nProphetModel (it must have copied me from last round ;))\n\nExponential Smoothing"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#my-solution",
    "href": "posts/2023-06-02-man-vs-ml/index.html#my-solution",
    "title": "Man vs Machine Learning",
    "section": "My solution",
    "text": "My solution\nGiven the multiplicative process here, I modeled the log transformed data. (I did try a more generalized Box-Cox transformation, but got better performance with a straight natural log transform). I tried an ARIMA model, using model selection via the Hyndman-Khandakar algorithm8, which resulted in a ARIMA(2,0,1)(1,1,2)[12] w/ drift&gt;."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#results-1",
    "href": "posts/2023-06-02-man-vs-ml/index.html#results-1",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nYay! I won this round. Quite easily.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n2.43\n\n\nAzure AutoML\nMAPE\n9.22\n\n\nMe\nRMSE\n1.63\n\n\nMe\nMAPE\n7.23"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#references",
    "href": "posts/2023-06-02-man-vs-ml/index.html#references",
    "title": "Man vs Machine Learning",
    "section": "References",
    "text": "References\nO’Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‘tsibble’. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#other",
    "href": "posts/2023-06-02-man-vs-ml/index.html#other",
    "title": "Man vs Machine Learning",
    "section": "Other",
    "text": "Other\nThanks to the Tidyverts team https://tidyverts.org/. The new an improved time series stack in R makes all this so easy.\nNote: None of this was super-rigorous, and I certainly tilted the board in my favour here and there. It was just fun and a chance to play around with a tool that I have previously avoided for no real reason."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml/index.html#footnotes",
    "href": "posts/2023-06-02-man-vs-ml/index.html#footnotes",
    "title": "Man vs Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Australian Energy Market Operator; tsibbledata R package↩︎\nYoung, P. C., Pedregal, D. J., & Tych, W. (1999). Dynamic harmonic regression. Journal of Forecasting, 18, 369–394. https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-131X(199911)18:6%3C369::AID-FOR748%3E3.0.CO;2-K↩︎\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05.↩︎\nTaylor SJ, Letham B. 2017. Forecasting at scale. PeerJ Preprints 5:e3190v2 https://doi.org/10.7287/peerj.preprints.3190v2↩︎\nFan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-parametric additive model. IEEE Transactions on Power Systems, 27(1), 134–141. https://ieeexplore.ieee.org/document/5985500↩︎\nHyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak electricity demand. IEEE Transactions on Power Systems, 25(2), 1142–1153. https://ieeexplore.ieee.org/document/5345698↩︎\nSource: Medicare Australia; tsibbledata R package↩︎\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(1), 1–22. https://doi.org/10.18637/jss.v027.i03↩︎"
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html",
    "href": "posts/2023-09-11-enterprise-ml/index.html",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "",
    "text": "Are you about to scale up your data analytics team to do more AI/ML work? Here are 5 things you need to know up front to make your life easier.\nThese are tips focused on enterprise-level organisations wanting to implement Microsoft’s Azure Machine Learning studio. But I’m sure the content translates well to other settings."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#people---process---platform-pick-any-three",
    "href": "posts/2023-09-11-enterprise-ml/index.html#people---process---platform-pick-any-three",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "1) People - Process - Platform (pick any three)",
    "text": "1) People - Process - Platform (pick any three)\nEffective analytics and machine learning requires coordination of People, Process & Platform. But do you need all three?\nYes. But you don’t need all three to get started.\nMost Data Scientists will not care about and will not identify as owning MLOps. They want to lead experimentation and innovation, not building production-grade ML pipelines. You will either have to:\n\nFind an internal champion to lead this and train others.\nHire an ML engineer if you have enough work to keep them busy (A recent ML Engineer job ad I saw was around $650k, so bear that in mind).\n\nWork with a specialist partner / consultant to fill this gap and support your teams doing what they are good at.\n\nConverting experimental analytics POCs into robust MLOps-style pipelines is not trivial. It takes real, behind the scenes work which is kind of thankless. But sometimes you need to eat your vegetables and do this. Ensuring you have a well crafted process for governing how, when and why MLOps should exist will help secure the time, funding and social capital you need."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#dont-overlook-security",
    "href": "posts/2023-09-11-enterprise-ml/index.html#dont-overlook-security",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "2) Don’t overlook security",
    "text": "2) Don’t overlook security\nWant to know where you are going to face challenges and delays implementing an AI/Ml platform?\nWonder no more: Network Security.\nAzure ML Studio (and any ML platform) will require serious planning around network architecture and security, and for good reason. It’s not as scary as it sounds, but it does require expertise. You need to have strong representation on your project from a decision maker and a ‘doer’ in network security. If these people aren’t inside the tent with you, your project will grind to a halt."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#think-about-dev-work-not-just-mlops",
    "href": "posts/2023-09-11-enterprise-ml/index.html#think-about-dev-work-not-just-mlops",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "3) Think about dev work, not just MLOps",
    "text": "3) Think about dev work, not just MLOps\nIf we are being honest, not many organisations are ready for scalable, reproducible, high performance ML deployments. But almost all orgs are building toward this in the next 5 years. So what’s most useful in the here and now?\n\nEasy to access online notebooks with various kernels\nClick of a button scalable compute, without fussing about with the infra layer\nAutoML tools to find performance ceilings and explore the solution space of new problems\n\nWant to get your data scientist on board? Tell them they wont have to run jobs on their laptops overnight anymore."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#its-python-heavy-sorry-r-users",
    "href": "posts/2023-09-11-enterprise-ml/index.html#its-python-heavy-sorry-r-users",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "4) It’s Python heavy (sorry R users)",
    "text": "4) It’s Python heavy (sorry R users)\nThere a lot to complain about if you are an R user. Azure ML studio is decidedly a Python oriented tool. While admittedly, most high-level AI development work is in Python, in reality a lot of so called “ML” projects are classical statistical models anyway. There are nice tools within R ecosystem for putting R in prod. However if you work in an enterprise setting, you may need to deploy your models on the chosen enterprise analytics platform. This is not ideal for R users, but hopefully in a future post I can show you how to have the best of both worlds."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#its-not-an-etl-environment",
    "href": "posts/2023-09-11-enterprise-ml/index.html#its-not-an-etl-environment",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "5) Its not an ETL environment",
    "text": "5) Its not an ETL environment\nLike any tool its only good for what it’s good for. When we think of true end-to-end machine learning projects there are steps in that process that are better fit in other tools. A common suspect here is the data import and processing. It’s nice to coordinate ML deployments with your data engineering teams to ensure you are using the right tools for the right jobs."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#want-to-learn-more",
    "href": "posts/2023-09-11-enterprise-ml/index.html#want-to-learn-more",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "Want to learn more?",
    "text": "Want to learn more?\nIf you are wanting to take the next steps in setting up people, process and platforms for high performance machine learning, get in touch for a chat."
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "",
    "text": "A recent project with repeated measures data involved fitting a random intercept term, and eventually making predictions for new groups not in the training sample. Importantly there was a need for individual predictions rather than population mean level predictions. Now, you obviously cannot include the random effect for a level that is not in your data, so the idea was to make a population level prediction with an adequate prediction interval that reflected the variation from both the fixed and random effects. This is complicated.\nIn the help page for lme4::predict.merMod() is the following note:\nThere are some useful resources out there but it took a while to track down, so this post may serve as a good reference in the future.\nLet’s go through an example using the famous sleepstudy data showing the average reaction time per day (in milliseconds) for subjects in a sleep deprivation study.\nlibrary(lme4)\nlibrary(tidyverse)\n\ndata(\"sleepstudy\")"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Model",
    "text": "Linear Model\nWe would like to model the relationship between Reaction and Days\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  theme_bw()\n\n\n\n\nFitting a basic linear model:\n\nfit_lm &lt;- lm(Reaction ~ Days, data = sleepstudy)\n\nsummary(fit_lm)\n\n\nCall:\nlm(formula = Reaction ~ Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,    Adjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n\n\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw()\n\n\n\n\nBut this ignores the fact these data are not independent. We have multiple observation per subject. Some look like a good fit, others not.\n\nggplot(sleepstudy, aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Mixed Effects Model",
    "text": "Linear Mixed Effects Model\nLet’s add a random intercept term for Subject. For simplicity we will leave out any other random effects.\n\nfit &lt;- lme4::lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)\n\nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nNew fitted lines can be drawn, showing the adjusted intercept for each subject (original regression line kept for reference).\n\nsleepstudy |&gt; \n  mutate(pred = predict(fit, re.form = NULL)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], col = \"grey\") +\n  geom_line(aes(Days, pred), show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Bootstrapped Prediction Intervals (observed data)",
    "text": "Bootstrapped Prediction Intervals (observed data)\nLet’s try and generate prediction intervals using lme4::bootMer() as suggested.\nFirst on the in-sample data.\n\n# predict function for bootstrapping\npredfn &lt;- function(.) {\n  predict(., newdata=new, re.form=NULL)\n}\n\n# summarise output of bootstrapping\nsumBoot &lt;- function(merBoot) {\n  return(\n    data.frame(fit = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.5, na.rm=TRUE))),\n               lwr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.025, na.rm=TRUE))),\n               upr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.975, na.rm=TRUE)))\n    )\n  )\n}\n\n# 'new' data\nnew &lt;- sleepstudy\n\nNotes:\n\nIn the predict() function we specify re.form=NULL which identifies which random effects to condition on. Here NULL includes all random effects. Obviously here you can compute individual predictions assuming you feed it with the correct grouping level in your data.\nIn the lme4::bootMer() function we set use.u=TRUE. This conditions on the random effects and only provides uncertainly estimates for the i.i.d. errors resulting from the fixed effects of the model.\n\n\nIf use.u is TRUE and type==“parametric”, only the i.i.d. errors are resampled, with the values of u staying fixed at their estimated values.\n\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=TRUE, type=\"parametric\")\n\n\nnew |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject, fill = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Dealing with unobserved data",
    "text": "Dealing with unobserved data\nHowever, this gets complicated if we want to make predictions for new subjects.\nWe can no longer condition on the random effects, as the new subject level will not have a fitted random intercept value. Instead we need to effectively make a population level prediction (i.e. set the random effect to zero.). This makes sense as we don’t know what the random effect ought to be for a given, unobserved subject.\nBut we don’t want the prediction interval to just cover the uncertainty in the population level estimate. If we are interested in individual predictions, how can we incorporate the uncertainly of the random effects in the prediction intervals?\nLets generate a new, unobserved subject.\n\nnew_subject &lt;- tibble(\n  Days = 0:9,\n  Subject = factor(\"999\")\n  )\n\nWe provide a new predict function that doesn’t condition on the random effects by using re.form = ~0. This lets us input and obtain predictions for new subjects.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\n\nnew_subject |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nHowever using predict just results in a completely deterministic prediction as shown above.\nAn alternative approach is to use lme4::simulate() which will simulate responses for subjects non-deterministically using the fitted model object.\nBelow we can see a comparison on both approaches.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\nsfun &lt;- function(.) {\n    simulate(., newdata=new_subject, re.form=NULL, allow.new.levels=TRUE)[[1]]\n}\n\n\nnew_subject |&gt; \n  bind_cols(simulated = sfun(fit)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  pivot_longer(cols = c(3, 4), names_to = \"type\", values_to = \"val\") |&gt; \n  ggplot(aes(Days, val, col = type)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nWe can use this simulate() function in our bootstrapping to resample responses from the fitted model (rather than resampling deterministic population predictions).\nThis time we set use.u=FALSE to provide uncertainly estimates from both the model errors and the random effects.\n\nIf use.u is FALSE and type is “parametric”, each simulation generates new values of both the “spherical” random effects uu and the i.i.d. errors , using rnorm() with parameters corresponding to the fitted model x.\n\n\nboot &lt;- lme4::bootMer(fit, sfun, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nSo while we don’t have a conditional mode of the random effect (because its a new subject) we can derive a bootstrapped estimate of the prediction interval by resampling the random effects and model errors on simulated data values."
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Aside",
    "text": "Aside\nFor comparison, here is what the same prediction interval would look like if we just used an unconditional population prediction. While the overall gist is the same, despite also resampling both the random effects and the i.i.d. errors, the interval is narrower as it is resampling just the deterministic population predictions of the model.\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "References",
    "text": "References\nMost of the material and code is taken from a variety of sources below. In particular the lme4 github issue. Also, the merTools package has a nice vignette comparing these methods with their own solution.\nhttps://tmalsburg.github.io/predict-vs-simulate.html https://github.com/lme4/lme4/issues/388 https://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-confidence-or-prediction-intervals-on-predictions"
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "",
    "text": "In a previous post I linked to a project that makes it easy to deploy and extend an existing Rocker Project Docker image to quickly set up a fully featured RStudio Server environment locally on your machine.\nHere I’ll cover some options to deploy this environment to the cloud so you can access it anywhere."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 1: Deploy to a Virtual Machine",
    "text": "Option 1: Deploy to a Virtual Machine\nA common pattern is to create a Virtual Machine (VM) with a cloud service provider (such as AWS, Azure, GCP) and run your code there. I’ll cover an example using Microsoft Azure.\n\nDeploy a VM with an Ubuntu operating system. Go ahead and choose the compute power you need.\n\n\n\nConfigure a custom network rule to allow traffic on port 8787 for RStudio\n\n\n3. Log into your new VM terminal using SSH\n\n\nInstall Docker Engine by following these steps\nClone and Deploy the docker container from Step 2 in my guide."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 2: Deploy using Azure App Service",
    "text": "Option 2: Deploy using Azure App Service\nThe above is fine, but arguably if you are setting up a VM from scratch for development purposes I’m not sure what benefit there is from using a docker container. You may as well just directly install what you want and consider the VM a ‘container’.\nHowever, if you plan to make this available to other users in your organisation, or to adapt this guide for Shiny App development you may be interested in other features such as TLS/SSL security, scale up, advanced networking, continuous integration, continuous deployment, staging/production deployment slots etc. This represents a shift from development sandpit to ‘web app’. For this case, Azure App Service may be a lower hassle option. This is Microsoft’s enterprise grade, web app deployment managed service.\nIn the Virtual Machine model you are setting up compute infrastructure, deploying and running containers directly - then fiddling with the infrastructure layer for everything else. In App Service you deploy your custom docker container (here containing RStudio Server) to Azure Container Registry (kind of like DockerHub). Azure App Services then builds and serves your app from there - without you having to stand up and manage an Infra layer directly.\n\n\nCreate Azure Container Registry (ACR) (or some other Docker repository) using this help guide\nRun and test your container locally\nDeploy your local container to ACR using this help guide\nCreate a new web app in Azure App Services using this help guide\nConfiguration:\n\n\nI didn’t have to fiddle with ports, presumably it reads the exposed ports in the docker file and does this magically.\n\nFor custom environment variables like the RStudio Server password, I had to manually add this in the config section.\n\n\nand it worked just fine:"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Deploying Enterprise Scale AI & Machine Learning Infrastucture\n\n\n5 things you need to know before using Azure ML Studio\n\n\n\n\nanalysis\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nThree Questions to Ask Your Data Scientist\n\n\nAre your predictive models doing what you think they are?\n\n\n\n\nanalysis\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nWhy your data science projects are failing\n\n\nand what you can’t do about it\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nMan vs Machine Learning\n\n\nI went head-to-head with Microsoft’s AutoML platform in a predictive modelling challenge.\n\n\n\n\nR\n\n\nAzure\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nLevel up your Forecasting\n\n\nQuantify uncertainty with distributional forecasts\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nRandom Number Generator Testing\n\n\nA tale of an unemployed ice-cream man, secret societies and a $10 radio\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nDeploy Your Own R Data Science Lab in the Cloud\n\n\n\n\n\n\n\nR\n\n\nazure\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nBuilding your own R Data Science Lab in the browser\n\n\n\n\n\n\n\nR\n\n\ndocker\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nBeware of Boundaries in Binominal Proportion Confidence Intervals\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nWhen should you be using the Hypergeometric distribution in practice?\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nPrediction Intervals for Linear Mixed Effects Models\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nRunning Shiny in a Docker container\n\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "href": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "title": "Beware of Boundaries in Binominal Proportion Confidence Intervals",
    "section": "",
    "text": "Binomial proportion confidence intervals are often employed when attempting to perform tests for significance, or sample size calculations around sample measurements resulting from a Bernoulli process.\nThe typical choice when calculating binomial proportion confidence intervals is the asymptotic, or normally approximated ‘Wald’ interval where success probability is measured by:\n\\[\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nIn many settings, such as marketing analytics or manufacturing processes the sample proportion is close to 0 or 1. Evaluating asymptotic confidence intervals near these boundary conditions will lead to underestimation of the error, and in some cases producing an interval outside \\([0, 1]\\).\nFortunately other methods exist, such as Wilson’s score interval, exact methods and Bayesian approaches. The recommendation here is to examine the probability coverage and explore alternative methods for sample size and CI calculation, especially when the parameter is near the boundary conditions, or in cases of very small n. \n\nlibrary(binom)\nlibrary(tidyverse)\n\nn &lt;- 50\np &lt;- c(0.01, 0.5, 0.99)\n\n\nx &lt;- purrr::map_df(p, .f =  ~binom.confint(x = n * .x, n = n, methods = 'all'))\n\n\nggplot(x, aes(colour = factor(x))) +\n  geom_point(aes(mean, method), show.legend = F) +\n  geom_errorbarh(aes(xmin = lower, xmax = upper, y = method), show.legend = F) +\n  geom_vline(xintercept =  c(0, 1), lty = 2, col = \"grey\") +\n  facet_wrap(~(x*2/100)) +\n  theme_bw() +\n  labs(title = \"A variety of binomial confidence interval methods for p = 0.01, 0.5 & 0.99\",\n       subtitle = \"Note unusual behaviour near 0.01 and 0.99\")\n\n\n\n\n\ncov &lt;- purrr::map_df(p, ~binom.coverage(.x, n, conf.level = 0.95, method = \"all\"))\n\n\nggplot(cov, aes(colour = factor(p))) +\n  geom_point(aes(coverage, method), show.legend = F) +\n  geom_vline(xintercept =  0.95, lty = 2) +\n  facet_wrap(~(p)) +\n  theme_bw() +\n  labs(title = \"Probability coverage for a variety of binomial confidence interval methods\",\n       subtitle = \"Reference line at 0.95 coverage\")\n\n\n\n\nA good discussion is contained in:\nWallis, Sean A. (2013). “Binomial confidence intervals and contingency tests: mathematical fundamentals and the evaluation of alternative methods” (PDF). Journal of Quantitative Linguistics. 20 (3): 178–208. doi:10.1080/09296174.2013.799918. S2CID 16741749.\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Statistical Consulting",
    "section": "",
    "text": "All science, no hype."
  }
]